{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nimport os \ncsv_path='/kaggle/input/creditcardfraud/creditcard.csv'\nprint(\"Csv path fetched.\")\nprint(f\"path : {csv_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:29.783565Z","iopub.execute_input":"2025-08-01T13:52:29.783956Z","iopub.status.idle":"2025-08-01T13:52:30.310537Z","shell.execute_reply.started":"2025-08-01T13:52:29.783920Z","shell.execute_reply":"2025-08-01T13:52:30.309548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv(csv_path)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:30.312092Z","iopub.execute_input":"2025-08-01T13:52:30.312666Z","iopub.status.idle":"2025-08-01T13:52:34.699223Z","shell.execute_reply.started":"2025-08-01T13:52:30.312642Z","shell.execute_reply":"2025-08-01T13:52:34.698403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:34.700073Z","iopub.execute_input":"2025-08-01T13:52:34.700362Z","iopub.status.idle":"2025-08-01T13:52:35.150686Z","shell.execute_reply.started":"2025-08-01T13:52:34.700332Z","shell.execute_reply":"2025-08-01T13:52:35.149891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.152476Z","iopub.execute_input":"2025-08-01T13:52:35.152741Z","iopub.status.idle":"2025-08-01T13:52:35.203858Z","shell.execute_reply.started":"2025-08-01T13:52:35.152720Z","shell.execute_reply":"2025-08-01T13:52:35.203012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### So We are going to predict the class feature using classification algorithm","metadata":{}},{"cell_type":"code","source":"df['Class'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.204743Z","iopub.execute_input":"2025-08-01T13:52:35.205095Z","iopub.status.idle":"2025-08-01T13:52:35.215178Z","shell.execute_reply.started":"2025-08-01T13:52:35.205073Z","shell.execute_reply":"2025-08-01T13:52:35.214360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Class'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.216167Z","iopub.execute_input":"2025-08-01T13:52:35.216451Z","iopub.status.idle":"2025-08-01T13:52:35.233747Z","shell.execute_reply.started":"2025-08-01T13:52:35.216431Z","shell.execute_reply":"2025-08-01T13:52:35.232834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Very Imbalanced Dataset ","metadata":{}},{"cell_type":"markdown","source":"##### And the feature from v1 to v28 are already scaled and transfromed using principal component analysis ","metadata":{}},{"cell_type":"code","source":"df.loc[df['Class']==1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.234666Z","iopub.execute_input":"2025-08-01T13:52:35.235020Z","iopub.status.idle":"2025-08-01T13:52:35.267868Z","shell.execute_reply.started":"2025-08-01T13:52:35.234992Z","shell.execute_reply":"2025-08-01T13:52:35.267163Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Time represents the number of seconds elapsed between a transaction and the first transaction in the dataset.","metadata":{}},{"cell_type":"markdown","source":"### So this data is imbalanced and their are no null values with all the datatypes are correctly arranged and also no visualization required because it is already transfromed and scaled so directly model building is only option required and just the model dosent need of time feature and also may be lets check it ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split,TimeSeriesSplit,StratifiedKFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.268542Z","iopub.execute_input":"2025-08-01T13:52:35.268741Z","iopub.status.idle":"2025-08-01T13:52:35.284512Z","shell.execute_reply.started":"2025-08-01T13:52:35.268725Z","shell.execute_reply":"2025-08-01T13:52:35.283616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_check=df.copy()\ntype(df_check)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.285575Z","iopub.execute_input":"2025-08-01T13:52:35.285880Z","iopub.status.idle":"2025-08-01T13:52:35.318756Z","shell.execute_reply.started":"2025-08-01T13:52:35.285854Z","shell.execute_reply":"2025-08-01T13:52:35.318002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_check = df.copy()  # assuming df has the full dataset with 'Class' as the target\nX = df_check.drop('Class', axis=1).values\ny = df_check['Class'].values\n\n# Time-based split\ntime_split = TimeSeriesSplit(n_splits=10)\n# Strarifed Based Split\nskf=StratifiedKFold(n_splits=10)\n\nfor train_idx, val_idx in time_split.split(X):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\nfor train_idx, val_idx in skf.split(X,y):\n    X_train_skf, X_val_skf = X[train_idx], X[val_idx]\n    y_train_skf, y_val_skf = y[train_idx], y[val_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:35.321672Z","iopub.execute_input":"2025-08-01T13:52:35.322181Z","iopub.status.idle":"2025-08-01T13:52:36.053910Z","shell.execute_reply.started":"2025-08-01T13:52:35.322161Z","shell.execute_reply":"2025-08-01T13:52:36.053244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split:\n#### 1. Stratified Fold Split\n#### 2. Time Series Split\n# Model :\n#### 1. Logistic Regression\n#### 2. Decision tree Classifier \n#### 3. Random Forest Classifier\n#### 4. XGBoost Classifier\n# Metric\n#### 1. Precision\n#### 2. Recall  # This we want to increase \n#### 3. Precision Recall Trade-Off\n#### 4. ROC-AUC Curve # TO check Model Stability\n# Cross Validation ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:36.054850Z","iopub.execute_input":"2025-08-01T13:52:36.055109Z","iopub.status.idle":"2025-08-01T13:52:36.873432Z","shell.execute_reply.started":"2025-08-01T13:52:36.055091Z","shell.execute_reply":"2025-08-01T13:52:36.872636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nlog_reg=LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train,y_train)\ny_train_pred=log_reg.predict(X_train)\nprint(f\"Classification Report : \\n{classification_report(y_train,y_train_pred)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:36.874342Z","iopub.execute_input":"2025-08-01T13:52:36.874620Z","iopub.status.idle":"2025-08-01T13:52:48.889497Z","shell.execute_reply.started":"2025-08-01T13:52:36.874600Z","shell.execute_reply":"2025-08-01T13:52:48.888723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score, precision_recall_curve, roc_curve, auc\n\ndef plot_fig(model, X_train, y_train, y_pred_proba):\n    # Convert probabilities to class predictions using 0.5 threshold (or another)\n    y_pred = (y_pred_proba >= 0.5).astype(int)\n\n    precision = precision_score(y_train, y_pred)\n    recall = recall_score(y_train, y_pred)\n    auc_score = roc_auc_score(y_train, y_pred_proba)\n\n    print(\"\\n*** Metrics ***\")\n    print(f\"Precision : {precision:.4f}\")\n    print(f\"Recall    : {recall:.4f}\")\n    print(f\"ROC AUC   : {auc_score:.4f}\")\n\n    # Precision-Recall Curve\n    precisions, recalls, thresholds = precision_recall_curve(y_train, y_pred_proba)\n\n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_train, y_pred_proba)\n\n    # Plotting\n    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n\n    # Precision-Recall Tradeoff vs Threshold\n    ax[0].plot(thresholds, precisions[:-1], 'r--', label='Precision')\n    ax[0].plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    ax[0].set_xlabel(\"Threshold\")\n    ax[0].set_ylabel(\"Score\")\n    ax[0].set_title(\"Precision-Recall Tradeoff\")\n    ax[0].legend()\n\n    # Precision vs Recall\n    ax[1].plot(recalls, precisions, color='blue')\n    ax[1].set_xlabel(\"Recall\")\n    ax[1].set_ylabel(\"Precision\")\n    ax[1].set_title(\"Precision vs Recall\")\n\n    # ROC Curve\n    ax[2].plot(fpr, tpr, color='darkorange', label=f\"AUC = {auc_score:.2f}\")\n    ax[2].plot([0, 1], [0, 1], linestyle='--', color='gray')\n    ax[2].set_xlabel(\"False Positive Rate\")\n    ax[2].set_ylabel(\"True Positive Rate\")\n    ax[2].set_title(\"ROC Curve\")\n    ax[2].legend(loc='lower right')\n\n    plt.tight_layout()\n    plt.show()\n    return thresholds,precisions,recalls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:48.890286Z","iopub.execute_input":"2025-08-01T13:52:48.890619Z","iopub.status.idle":"2025-08-01T13:52:48.899560Z","shell.execute_reply.started":"2025-08-01T13:52:48.890596Z","shell.execute_reply":"2025-08-01T13:52:48.898679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"thresholds,precisions,recalls=plot_fig(log_reg,X_train,y_train,log_reg.predict_proba(X_train)[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:48.900365Z","iopub.execute_input":"2025-08-01T13:52:48.900646Z","iopub.status.idle":"2025-08-01T13:52:51.908964Z","shell.execute_reply.started":"2025-08-01T13:52:48.900629Z","shell.execute_reply":"2025-08-01T13:52:51.908014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nbest_thresh = thresholds[np.argmax(recalls >= 0.80)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:51.909929Z","iopub.execute_input":"2025-08-01T13:52:51.910161Z","iopub.status.idle":"2025-08-01T13:52:51.914727Z","shell.execute_reply.started":"2025-08-01T13:52:51.910143Z","shell.execute_reply":"2025-08-01T13:52:51.913737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tree=DecisionTreeClassifier()\ntree.fit(X_train,y_train)\ny_tr_score=tree.predict(X_train)\n_,_,_=plot_fig(tree,X_train,y_train,tree.predict_proba(X_train)[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:52:51.915449Z","iopub.execute_input":"2025-08-01T13:52:51.915761Z","iopub.status.idle":"2025-08-01T13:53:16.868440Z","shell.execute_reply.started":"2025-08-01T13:52:51.915735Z","shell.execute_reply":"2025-08-01T13:53:16.867628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_tree = RandomForestClassifier()\nrf_tree.fit(X_train, y_train)\n\ny_tr_score = rf_tree.predict(X_train)\n\nthresholds, precisions, recalls = plot_fig(\n    rf_tree, X_train, y_train, rf_tree.predict_proba(X_train)[:, 1]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:53:16.869353Z","iopub.execute_input":"2025-08-01T13:53:16.869619Z","iopub.status.idle":"2025-08-01T13:58:36.103184Z","shell.execute_reply.started":"2025-08-01T13:53:16.869600Z","shell.execute_reply":"2025-08-01T13:58:36.102260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### So in this case the random forest and decision tree are completely fitting the data so first i am checking the correlation to confirm or just go with logistic regression and increase the recall by tuning ","metadata":{}},{"cell_type":"code","source":"df_copy=df.copy()\ncorr=df_copy.corr()['Class'].sort_values(ascending=False)\nprint(f\"Top Correlation with respect to Class : \\n{corr}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:36.104287Z","iopub.execute_input":"2025-08-01T13:58:36.104620Z","iopub.status.idle":"2025-08-01T13:58:36.972834Z","shell.execute_reply.started":"2025-08-01T13:58:36.104591Z","shell.execute_reply":"2025-08-01T13:58:36.972015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### As this is already dimension reduced data set so their is only one option to go for logistic regression","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\nlog_reg.fit(X_train,y_train)\ny_train_pred=log_reg.predict(X_train)\nprint(f\"Classification Report : \\n{classification_report(y_train,y_train_pred)}\")\nthresholds,precisions,recalls=plot_fig(log_reg,X_train,y_train,log_reg.predict_proba(X_train)[:,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:36.973688Z","iopub.execute_input":"2025-08-01T13:58:36.974018Z","iopub.status.idle":"2025-08-01T13:58:41.754887Z","shell.execute_reply.started":"2025-08-01T13:58:36.973999Z","shell.execute_reply":"2025-08-01T13:58:41.754122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Just creation test for random 10 sample\ndict_test={}\nfor i in range(10):\n    smp=np.random.randint(len(df.index))\n    y_true=df.iloc[smp,-1]\n    y_pred = log_reg.predict(df.iloc[smp, :-1].values.reshape(1, -1))\n    dict_test[f\"iterate_{i}-{smp}\"]=[y_true,y_pred]\n    \nprint(f\"The Random Sample prediction and the true values\")\ndict_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:41.755844Z","iopub.execute_input":"2025-08-01T13:58:41.756155Z","iopub.status.idle":"2025-08-01T13:58:41.769473Z","shell.execute_reply.started":"2025-08-01T13:58:41.756129Z","shell.execute_reply":"2025-08-01T13:58:41.768723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[273143][-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:41.770218Z","iopub.execute_input":"2025-08-01T13:58:41.770490Z","iopub.status.idle":"2025-08-01T13:58:41.790223Z","shell.execute_reply.started":"2025-08-01T13:58:41.770471Z","shell.execute_reply":"2025-08-01T13:58:41.789349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Okay so it seems that mostly data is valid just check by confusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\ncm=confusion_matrix(y_train,log_reg.predict(X_train))\nConfusionMatrixDisplay(cm).plot()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:41.791034Z","iopub.execute_input":"2025-08-01T13:58:41.791366Z","iopub.status.idle":"2025-08-01T13:58:42.044298Z","shell.execute_reply.started":"2025-08-01T13:58:41.791339Z","shell.execute_reply":"2025-08-01T13:58:42.043603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Status : (logistic Regression)')\nprint(f\"True Negative : {cm[0][0]}\")\nprint(f\"True Positive : {cm[1][1]}\")\nprint(f\"False Positive : {cm[0][1]}\")\nprint(f\"False Negative : {cm[1][0]}\")\nprint(\"--\"*40)\nprint(f\"ratio of true negative : {cm[0][0]/df.shape[0]}\")\nprint(f\"ratio of true positive : {cm[1][1]/df.shape[0]}\")\nprint(f\"ratio of False positive : {cm[0][1]/df.shape[0]}\")\nprint(f\"ratio of False negative : {cm[1][0]/df.shape[0]}\")\nprint(\"--\"*40)\nprint(f\"Ratio of true positive by all positive : {cm[1][1]/(cm[1][1]+cm[0][1])}\")\nprint(f\"Ratio of false positive by all positive : {cm[0][1]/(cm[1][1]+cm[0][1])}\")\nprint(f\"Ratio of true negative by all positive : {cm[0][0]/(cm[0][0]+cm[1][0])}\")\nprint(f\"Ratio of false negative by all positive : {cm[1][0]/(cm[0][0]+cm[1][0])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:42.045101Z","iopub.execute_input":"2025-08-01T13:58:42.045363Z","iopub.status.idle":"2025-08-01T13:58:42.052441Z","shell.execute_reply.started":"2025-08-01T13:58:42.045336Z","shell.execute_reply":"2025-08-01T13:58:42.051578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Class'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:42.053184Z","iopub.execute_input":"2025-08-01T13:58:42.053453Z","iopub.status.idle":"2025-08-01T13:58:42.079777Z","shell.execute_reply.started":"2025-08-01T13:58:42.053434Z","shell.execute_reply":"2025-08-01T13:58:42.078918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Lets try to increase the threshold","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ncur_threshold = [0.15,0.25,0.5,0.65,0.75,0.85,0.95,1.0]\ncm_lst=[]\ni=0\nfor threshold in cur_threshold:\n    print(f\"Iteration {i}\")\n    y_proba = log_reg.predict_proba(X_train)[:, 1]\n    y_pred_thresh = (y_proba >= threshold).astype(int)\n    cm = confusion_matrix(y_train, y_pred_thresh)\n    cm_lst.append(cm)\n    print(\"F1 Score:\", f1_score(y_train, y_pred_thresh))\n    print(\"Precision:\", precision_score(y_train, y_pred_thresh))\n    print(\"Recall:\", recall_score(y_train, y_pred_thresh))\n    i+=1\n    print('--'*40)\n\nfig, ax = plt.subplots(ncols=4, nrows=2, figsize=(14, 8))  \nfor i in range(2):\n    for j in range(4):\n        idx = i * 4 + j\n        cm = cm_lst[idx]\n        threshold = cur_threshold[idx]\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n        disp.plot(ax=ax[i][j], colorbar=False)\n        ax[i][j].set_title(f\"Thresh: {threshold:.2f}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:58:42.080805Z","iopub.execute_input":"2025-08-01T13:58:42.081407Z","iopub.status.idle":"2025-08-01T13:58:44.771428Z","shell.execute_reply.started":"2025-08-01T13:58:42.081378Z","shell.execute_reply":"2025-08-01T13:58:44.770675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify_with_threshold(model, sample, threshold=0.85):\n    proba = model.predict_proba(sample)[:, 1][0]\n    label = int(proba >= threshold)\n    return label, proba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:08:23.047287Z","iopub.execute_input":"2025-08-01T14:08:23.047939Z","iopub.status.idle":"2025-08-01T14:08:23.053098Z","shell.execute_reply.started":"2025-08-01T14:08:23.047914Z","shell.execute_reply":"2025-08-01T14:08:23.052180Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### So for this above function tells that for each newsample model is trained and its probability (ex:. proba = 0.83 ) and it is compared with the threshold (as 0.83<0.85) so it is stated as class 0 ","metadata":{}},{"cell_type":"code","source":"## lets test on train sample\nsamp=df.iloc[12345,:-1]\nlabel,proba=classify_with_threshold(log_reg,[samp])\nprint(\"Label:\",label)\nprint(\"proba:\",proba)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:13:15.676989Z","iopub.execute_input":"2025-08-01T14:13:15.677897Z","iopub.status.idle":"2025-08-01T14:13:15.683682Z","shell.execute_reply.started":"2025-08-01T14:13:15.677871Z","shell.execute_reply":"2025-08-01T14:13:15.682793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### So model is now good to go lets convert to pkl . \n##### Note : This model is the data which we imported earlierr is already transformed to lower dimension thats why i hadan't done any scaling,selection of feature . So this model is not used for production because we didn't know what are the original feature","metadata":{"execution":{"iopub.status.busy":"2025-08-01T14:12:44.905478Z","iopub.execute_input":"2025-08-01T14:12:44.905800Z","iopub.status.idle":"2025-08-01T14:12:44.910799Z","shell.execute_reply.started":"2025-08-01T14:12:44.905778Z","shell.execute_reply":"2025-08-01T14:12:44.909766Z"}}},{"cell_type":"code","source":"import joblib\njoblib.dump('model','/kaggle/working/model.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:26.506258Z","iopub.execute_input":"2025-08-01T14:16:26.506675Z","iopub.status.idle":"2025-08-01T14:16:26.512961Z","shell.execute_reply.started":"2025-08-01T14:16:26.506653Z","shell.execute_reply":"2025-08-01T14:16:26.512112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}